# -*- coding: utf-8 -*-
"""model_training_mbert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Bo7QJLHY8EYQGnWzawMvZjrNZAoKRZW
"""

#install libraries
# !pip install  git+https://github.com/huggingface/transformers.git datasets accelerate scikit-learn -Uqq
# !pip install -U transformers>=4.48.0

import torch
import pandas as pd
import numpy as np
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    AutoConfig,
    TrainingArguments,
    Trainer,
)
import string
from sklearn.metrics import f1_score
import gc
from typing import Optional, Tuple, Dict


# Clear GPU memory
torch.cuda.empty_cache()
gc.collect()

# Load and preprocess the dataset
df = pd.read_csv('/content/drive/MyDrive/final_data_1.csv')

# Remove punctuation
def remove_punctuation(text:str) -> Optional[str]:
    """_Removes punctuation from the entire text column._

    Args:
        text (str): _the text column from the dataframe._

    Returns:
        Optional[str]: _a cleaned text or None._
    """
    return text.translate(str.maketrans('', '', string.punctuation))

def preprocess_and_split(df: pd.Dataframe = df) -> Tuple[pd.DataFrame]:
    """_Applies text cleaning preprocessing and performs train test
    split._

    Args:
        df (pd.Dataframe, optional): _dataframe to clean and split._. Defaults to df.

    Returns:
        Tuple[pd.DataFrame]: _Tuple of train_df and test_df._
    """
    df['text'] = df['text'].apply(remove_punctuation)
    # Split into train and test
    train_df, test_df = train_test_split(df, test_size=0.3, random_state=23)
    
    return train_df, test_df

train_df, test_df = preprocess_and_split()

# Convert to HuggingFace Dataset
train_data = Dataset.from_pandas(train_df)
test_data = Dataset.from_pandas(test_df)
train_data = train_data.remove_columns(["__index_level_0__"])
test_data = test_data.remove_columns(["__index_level_0__"])

dataset_dict = DatasetDict({
    'train': train_data,
    'test': test_data
})

# Tokenizer and model setup
tokenizer = AutoTokenizer.from_pretrained("answerdotai/ModernBERT-base")

# Reduce max_length to save memory
def tokenizer_function(example):
    return tokenizer(
        example["text"],
        padding="max_length",
        truncation=True,
        max_length=512,  # Reduced from 650
        return_tensors="pt"
    )

# Tokenize the dataset
tokenized_train_dataset = dataset_dict["train"].map(tokenizer_function, batched=True)
tokenized_test_dataset = dataset_dict["test"].map(tokenizer_function, batched=True)

# Print a sample
print(tokenized_test_dataset[0])

# Load model config and model for classification
config = AutoConfig.from_pretrained("answerdotai/ModernBERT-base", num_labels=2)
model = AutoModelForSequenceClassification.from_config(config)

# Prepare datasets
train_dataset = tokenized_train_dataset.remove_columns(['text']).rename_column('label', 'labels')
test_dataset = tokenized_test_dataset.remove_columns(['text']).rename_column('label', 'labels')

# Metric function
def compute_metrics(eval_pred) -> Dict:
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    score = f1_score(labels, predictions, average="weighted")
    return {"f1": score}

# Training arguments with memory-optimized params
training_args = TrainingArguments(
    output_dir="fine_tuned_modern_bert",
    learning_rate=5e-5,
    per_device_train_batch_size=8,  # Reduced from 32
    per_device_eval_batch_size=8,   # Reduced from 32
    num_train_epochs=3,
    lr_scheduler_type="linear",
    optim="adamw_torch",
    adam_beta1=0.9,
    adam_beta2=0.98,
    adam_epsilon=1e-6,
    weight_decay=8e-6,
    logging_strategy="epoch",
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    fp16=True,  # Mixed precision for memory efficiency
    gradient_checkpointing=True,  # Save GPU memory
    push_to_hub=False,
)

# Create Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
)

# Start training
trainer.train()

evaluation_results = trainer.evaluate()

print("Evaluation Results:", evaluation_results)

# Save the trained model
model.save_pretrained("./saved_model_ai")
# Save the tokenizer
tokenizer.save_pretrained("./saved_model_ai")

def predict(text):
    # Preprocess input text
    text = text.translate(str.maketrans('', '', string.punctuation))

    # Tokenize
    inputs = tokenizer(
        text,
        padding="max_length",
        truncation=True,
        max_length=512,
        return_tensors="pt"
    )

    # Move inputs to GPU if available
    if torch.cuda.is_available():
        inputs = {key: val.cuda() for key, val in inputs.items()}
        model.cuda()

    # Set model to evaluation mode
    model.eval()

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = torch.nn.functional.softmax(logits, dim=1).squeeze()
        predicted_class_id = probs.argmax().item()

    return (probs[0].item(), probs[1].item(), predicted_class_id)

if __name__ == "__main__":
    some_text = input('Paste: ')
    predict(some_text)

